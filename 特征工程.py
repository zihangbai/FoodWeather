import pandas as pd
import numpy as np
import os
import traceback
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif

# -------------------------- å…¨å±€å‡½æ•°ï¼šåŠ¨æ€åˆ†ç®±ï¼ˆè§£å†³ä½œç”¨åŸŸé—®é¢˜ï¼‰--------------------------
def dynamic_qcut(series, max_bins=3):
    """åŠ¨æ€åˆ†ç®±ï¼šå”¯ä¸€å€¼â‰¤max_binsæ—¶æŒ‰å”¯ä¸€å€¼åˆ†ç®±ï¼Œå¦åˆ™æŒ‰max_binsåˆ†ç®±ï¼Œæ ‡ç­¾ä¸º0~q-1"""
    unique_count = series.nunique()
    q = min(unique_count, max_bins) if unique_count > 0 else 1
    if q == 1:
        return pd.Series([0]*len(series), index=series.index)
    else:
        # é¦–å…ˆä¸æŒ‡å®šæ ‡ç­¾ï¼Œè®©pandasè‡ªåŠ¨åˆ†é…
        qcut_result = pd.qcut(series, q=q, duplicates='drop')
        # è·å–å®é™…çš„åˆ†ç®±æ•°é‡
        actual_bins = qcut_result.cat.categories.size
        # ç”Ÿæˆç›¸åº”æ•°é‡çš„æ ‡ç­¾
        labels = list(range(actual_bins))
        # é‡æ–°åˆ†é…æ ‡ç­¾
        qcut_result = qcut_result.cat.rename_categories(labels)
        return qcut_result


# -------------------------- 1. è¯»å–æ¸…æ´—åçš„æ•°æ®--------------------------
input_path = "D:/QQæ–‡æ¡£/æ•°æ®æŒ–æ˜/é£Ÿå“å¤©æ°”é¡¹ç›®/cleaned_dataset/"

customer_df = pd.read_excel(f"{input_path}cleaned_customer.xlsx")
orders_df = pd.read_excel(f"{input_path}cleaned_orders.xlsx")
product_df = pd.read_excel(f"{input_path}cleaned_product.xlsx")
weather_df = pd.read_excel(f"{input_path}cleaned_weather.xlsx")

print("âœ… æˆåŠŸè¯»å–æ¸…æ´—åæ•°æ®ï¼Œå¼€å§‹ç‰¹å¾å·¥ç¨‹...", flush=True)
print(f"å¤©æ°”æ•°æ®è¡Œæ•°: {len(weather_df)}", flush=True)
print(f"å¹³å‡æ¸©åº¦å”¯ä¸€å€¼æ•°é‡: {weather_df['avg_temperature'].nunique()}", flush=True)
print(f"å¹³å‡æ¸©åº¦å€¼: {weather_df['avg_temperature'].head()}", flush=True)


# -------------------------- 2. å®¢æˆ·ç‰¹å¾å·¥ç¨‹--------------------------
def build_customer_features(df):
    # åŸºç¡€ç‰¹å¾ç¼–ç 
    le_gender = LabelEncoder()
    df["Gender_Encode"] = le_gender.fit_transform(df["Gender"])
    
    # èœç³»ç‹¬çƒ­ç¼–ç 
    cuisine_dummies = pd.get_dummies(df["PreferredCuisine"], prefix="Cuisine")
    df = pd.concat([df, cuisine_dummies], axis=1)
    
    # å…¶ä»–åˆ†ç±»å˜é‡ç¼–ç 
    df["DiningOccasion_Encode"] = LabelEncoder().fit_transform(df["DiningOccasion"])
    df["TimeOfVisit_Encode"] = LabelEncoder().fit_transform(df["TimeOfVisit"])
    
    # åŠ¨æ€åˆ†ç®±ï¼ˆè°ƒç”¨å…¨å±€å‡½æ•°ï¼‰
    df["Spend_Level"] = dynamic_qcut(df["AverageSpend"], max_bins=3)
    df["Freq_Level"] = dynamic_qcut(df["VisitFrequency"], max_bins=3)
    
    # å®¢æˆ·ä»·å€¼æ€»åˆ†
    df["Customer_Value"] = df["Spend_Level"].astype(int) + df["Freq_Level"].astype(int)
    
    # æ»¡æ„åº¦ç›¸å…³ç‰¹å¾
    df["Overall_Rating"] = (df["ServiceRating"] + df["FoodRating"] + df["AmbianceRating"]) / 3
    df["HighSatisfaction_Encode"] = df["HighSatisfaction"].map({True: 1, False: 0}).fillna(0)
    
    return df

customer_features = build_customer_features(customer_df)
print(f"å®¢æˆ·ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç”Ÿæˆ {len(customer_features.columns)} ä¸ªå­—æ®µ", flush=True)


# -------------------------- 3. è®¢å•+èœå“ç‰¹å¾å·¥ç¨‹--------------------------
def build_order_product_features(orders_df, product_df):
    # åˆå¹¶è®¢å•ä¸èœå“å±æ€§
    order_product_df = orders_df.merge(product_df[["product_id", "é¤é¥®å“ç±»", "department"]], on="product_id", how="left")
    
    # èœå“è´­ä¹°é¢‘æ¬¡
    product_buy_count = order_product_df["product_id"].value_counts().reset_index()
    product_buy_count.columns = ["product_id", "Buy_Frequency"]
    order_product_df = order_product_df.merge(product_buy_count, on="product_id", how="left")
    
    # è®¢å•å†…å“ç±»å æ¯”
    order_category_ratio = order_product_df.groupby("order_id")["é¤é¥®å“ç±»"].value_counts(normalize=True).reset_index()
    order_category_ratio.columns = ["order_id", "é¤é¥®å“ç±»", "Category_Ratio"]
    order_product_df = order_product_df.merge(order_category_ratio, on=["order_id", "é¤é¥®å“ç±»"], how="left")
    
    # è®¢å•èœå“æ•°é‡
    order_size = order_product_df.groupby("order_id")["product_id"].count().reset_index()
    order_size.columns = ["order_id", "Order_Product_Count"]
    order_product_df = order_product_df.merge(order_size, on="order_id", how="left")
    
    # èœå“åˆ†ç±»ç¼–ç ï¼ˆå¡«å……ç¼ºå¤±å€¼ï¼‰
    order_product_df["department"] = order_product_df["department"].fillna("æœªçŸ¥")
    order_product_df["é¤é¥®å“ç±»"] = order_product_df["é¤é¥®å“ç±»"].fillna("æœªçŸ¥")
    order_product_df["Department_Encode"] = LabelEncoder().fit_transform(order_product_df["department"])
    order_product_df["é¤é¥®å“ç±»_Encode"] = LabelEncoder().fit_transform(order_product_df["é¤é¥®å“ç±»"])
    
    return order_product_df

order_product_features = build_order_product_features(orders_df, product_df)
print(f"è®¢å•+èœå“ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç”Ÿæˆ {len(order_product_features.columns)} ä¸ªå­—æ®µ", flush=True)


# -------------------------- 4. å¤©æ°”ç‰¹å¾å·¥ç¨‹ï¼ˆç°åœ¨èƒ½è°ƒç”¨å…¨å±€dynamic_qcutï¼‰--------------------------
def build_weather_features(weather_df):
    # æ¸©åº¦ç­‰çº§ï¼ˆè°ƒç”¨å…¨å±€åŠ¨æ€åˆ†ç®±å‡½æ•°ï¼‰
    weather_df["Temp_Level"] = dynamic_qcut(weather_df["avg_temperature"], max_bins=3)
    # é™æ°´æ ‡è¯†
    weather_df["Has_Precipitation"] = (weather_df["precipitation"] > 0).astype(int)
    # æ—¥ç…§å……è¶³æ ‡è¯†
    weather_df["Adequate_Sunlight"] = (weather_df["hours_sunlight"] > 6).astype(int)
    
    # åŒºåŸŸ-æ—¥æœŸèšåˆ
    weather_agg = weather_df.groupby(["åœ°åŒºå", "calendar_date"]).agg({
        "avg_temperature": "mean",
        "precipitation": "sum",
        "Has_Precipitation": "max",
        "Adequate_Sunlight": "max"
    }).reset_index()
    weather_agg.columns = ["åœ°åŒºå", "æ—¥æœŸ", "æ—¥å‡æ¸©åº¦", "æ€»é™æ°´é‡", "æ˜¯å¦é™æ°´", "æ—¥ç…§å……è¶³"]
    
    return weather_agg

weather_features = None
print("å¼€å§‹å¤©æ°”ç‰¹å¾å·¥ç¨‹...", flush=True)
try:
    weather_features = build_weather_features(weather_df)
    print(f"å¤©æ°”ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç”Ÿæˆ {len(weather_features.columns)} ä¸ªèšåˆå­—æ®µ", flush=True)
except Exception as e:
    print(f"å¤©æ°”ç‰¹å¾å·¥ç¨‹å‡ºé”™: {e}", flush=True)
    traceback.print_exc()
    # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨åŸå§‹æ•°æ®åˆ›å»ºä¸€ä¸ªç®€å•çš„weather_features
    weather_features = weather_df.groupby(["åœ°åŒºå"]).agg({
        "avg_temperature": "mean",
        "precipitation": "sum"
    }).reset_index()
    weather_features.columns = ["åœ°åŒºå", "æ—¥å‡æ¸©åº¦", "æ€»é™æ°´é‡"]
    print(f"å·²åˆ›å»ºç®€å•çš„å¤©æ°”ç‰¹å¾ï¼Œç”Ÿæˆ {len(weather_features.columns)} ä¸ªå­—æ®µ", flush=True)


# -------------------------- 5. å¤šæºç‰¹å¾èåˆ--------------------------
def merge_multi_source_features(customer_features, weather_features):
    # åŒºåŸŸå¤©æ°”å¹³å‡ç‰¹å¾
    region_weather_avg = weather_features.groupby("åœ°åŒºå").agg({
        "æ—¥å‡æ¸©åº¦": "mean",
        "æ€»é™æ°´é‡": "mean",
        "æ˜¯å¦é™æ°´": "mean",
        "æ—¥ç…§å……è¶³": "mean"
    }).reset_index()
    region_weather_avg.columns = ["åœ°åŒºå", "åŒºåŸŸå¹³å‡æ¸©åº¦", "åŒºåŸŸå¹³å‡é™æ°´é‡", "åŒºåŸŸé™æ°´æ¦‚ç‡", "åŒºåŸŸæ—¥ç…§å……è¶³æ¦‚ç‡"]
    
    # å…³è”å®¢æˆ·ä¸å¤©æ°”
    merged_features = customer_features.merge(region_weather_avg, left_on="åŒºåŸŸ", right_on="åœ°åŒºå", how="left")
    # å¡«å……æ— åŒ¹é…çš„å¤©æ°”å€¼
    weather_cols = ["åŒºåŸŸå¹³å‡æ¸©åº¦", "åŒºåŸŸå¹³å‡é™æ°´é‡", "åŒºåŸŸé™æ°´æ¦‚ç‡", "åŒºåŸŸæ—¥ç…§å……è¶³æ¦‚ç‡"]
    for col in weather_cols:
        merged_features[col] = merged_features[col].fillna(merged_features[col].mean())
    
    return merged_features

final_features = merge_multi_source_features(customer_features, weather_features)
print(f"å¤šæºç‰¹å¾èåˆå®Œæˆï¼Œæœ€ç»ˆç‰¹å¾é›†å…± {len(final_features.columns)} ä¸ªå­—æ®µ", flush=True)


# -------------------------- 6. ç‰¹å¾ç­›é€‰--------------------------
def select_core_features(df):
    # å®šä¹‰ç›®æ ‡å­—æ®µï¼Œç¡®ä¿ä¸ä¼šè¢«åˆ é™¤
    target_cols = ["Customer_Value", "HighSatisfaction_Encode"]
    
    # åˆ é™¤æ— æ•ˆå­—æ®µï¼Œä½†ä¿ç•™ç›®æ ‡å­—æ®µ
    drop_cols = [
        "CustomerID", "åœ°åŒºå", "å¼‚å¸¸æ ‡è®°", "HighSatisfaction", 
        "PreferredCuisine", "DiningOccasion", "TimeOfVisit", "Gender"
    ]
    # ç§»é™¤ç›®æ ‡å­—æ®µï¼ˆå¦‚æœå®ƒä»¬åœ¨drop_colsä¸­ï¼‰
    drop_cols = [col for col in drop_cols if col not in target_cols]
    df = df.drop(columns=[col for col in drop_cols if col in df.columns])
    
    # å»é™¤é«˜ç›¸å…³ç‰¹å¾ï¼ˆç›¸å…³ç³»æ•°>0.8ï¼‰ï¼Œä½†ä¿ç•™ç›®æ ‡å­—æ®µ
    corr_matrix = df.select_dtypes(include=["int64", "float64"]).corr()
    high_corr_cols = set()
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            # å¦‚æœä¸¤ä¸ªç‰¹å¾éƒ½ä¸æ˜¯ç›®æ ‡å­—æ®µï¼Œæ‰è€ƒè™‘åˆ é™¤
            col_i = corr_matrix.columns[i]
            col_j = corr_matrix.columns[j]
            if col_i not in target_cols and col_j not in target_cols:
                if abs(corr_matrix.iloc[i, j]) > 0.8:
                    high_corr_cols.add(col_j)
    df = df.drop(columns=high_corr_cols)
    
    # ç­›é€‰æ ¸å¿ƒç‰¹å¾ï¼ˆå®¹é”™ï¼šé¿å…ç‰¹å¾æ•°ä¸è¶³ï¼‰
    # ç¡®ä¿ç›®æ ‡å­—æ®µå­˜åœ¨
    if "Customer_Value" not in df.columns:
        # å¦‚æœCustomer_Valueä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ›¿ä»£å€¼
        df["Customer_Value"] = 0
        print("è­¦å‘Šï¼šCustomer_Valueå­—æ®µä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºé»˜è®¤å€¼", flush=True)
    
    X = df.select_dtypes(include=["int64", "float64"])
    y = df["Customer_Value"]
    k = min(20, len(X.columns)) if len(X.columns) > 0 else 1
    selector = SelectKBest(score_func=f_classif, k=k)
    X_selected = selector.fit_transform(X, y)
    selected_cols = X.columns[selector.get_support()].tolist()
    
    # ç¡®ä¿ç›®æ ‡å­—æ®µåœ¨ç»“æœä¸­
    for col in target_cols:
        if col in df.columns and col not in selected_cols:
            selected_cols.append(col)
    
    return df[selected_cols]

core_features = select_core_features(final_features)
print(f"ç‰¹å¾ç­›é€‰å®Œæˆï¼Œä¿ç•™ {len(core_features.columns)} ä¸ªæ ¸å¿ƒå­—æ®µï¼ˆå«ç›®æ ‡å˜é‡ï¼‰", flush=True)


# -------------------------- 7. ç‰¹å¾æ ‡å‡†åŒ–--------------------------
def standardize_features(df):
    target_cols = ["Customer_Value", "HighSatisfaction_Encode"]
    feature_cols = [col for col in df.columns if col not in target_cols and df[col].dtype in ["int64", "float64"]]
    
    if len(feature_cols) > 0:
        scaler = StandardScaler()
        df[feature_cols] = scaler.fit_transform(df[feature_cols])
    else:
        scaler = None  # æ— ç‰¹å¾å¯æ ‡å‡†åŒ–æ—¶è¿”å›None
    
    return df, scaler

standardized_features, scaler = standardize_features(core_features)
print("ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ", flush=True)


# -------------------------- 8. è¾“å‡ºç»“æœ--------------------------
output_path = "D:/QQæ–‡æ¡£/æ•°æ®æŒ–æ˜/é£Ÿå“å¤©æ°”é¡¹ç›®/feature_engineered_dataset/"
if not os.path.exists(output_path):
    os.makedirs(output_path)

# è¾“å‡ºæ‰€æœ‰ç‰¹å¾æ–‡ä»¶
customer_features.to_excel(f"{output_path}customer_features.xlsx", index=False, engine="openpyxl")
order_product_features.to_excel(f"{output_path}order_product_features.xlsx", index=False, engine="openpyxl")
weather_features.to_excel(f"{output_path}weather_features.xlsx", index=False, engine="openpyxl")
standardized_features.to_excel(f"{output_path}standardized_core_features.xlsx", index=False, engine="openpyxl")

print("ğŸ‰ ç‰¹å¾å·¥ç¨‹å…¨æµç¨‹100%å®Œæˆï¼æ— ä»»ä½•æŠ¥é”™å’Œè­¦å‘Šï¼", flush=True)
print(f"è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼š{output_path}", flush=True)
print("æ ¸å¿ƒæ–‡ä»¶è¯´æ˜ï¼š", flush=True)
print(f"1. customer_features.xlsx â†’ å®¢æˆ·åˆ†å±‚ã€ä»·å€¼é¢„æµ‹ä¸“ç”¨", flush=True)
print(f"2. order_product_features.xlsx â†’ èœå“å…³è”è§„åˆ™ã€é”€é‡åˆ†æä¸“ç”¨", flush=True)
print(f"3. weather_features.xlsx â†’ å¤©æ°”å¯¹æ¶ˆè´¹å½±å“åˆ†æä¸“ç”¨", flush=True)
print(f"4. standardized_core_features.xlsx â†’ ç›´æ¥ç”¨äºK-Meansã€å›å½’ç­‰å»ºæ¨¡", flush=True)