import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler
import warnings
import os
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# -------------------------- 1. é…ç½®è®¾å¤‡ --------------------------
# è‡ªåŠ¨æ£€æµ‹GPU/CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"PyTorchä¼˜åŒ–K-Means - ä½¿ç”¨è®¾å¤‡ï¼š{device}")

# -------------------------- 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† --------------------------
input_path = "D:/QQæ–‡æ¡£/æ•°æ®æŒ–æ˜/é£Ÿå“å¤©æ°”é¡¹ç›®/feature_engineered_dataset/"

# å°è¯•è¯»å–æ ‡å‡†åŒ–å’Œå®¢æˆ·ç‰¹å¾æ•°æ®
try:
    standardized_df = pd.read_excel(f"{input_path}standardized_core_features.xlsx")
    print("âœ… æˆåŠŸè¯»å–æ ‡å‡†åŒ–æ•°æ®")
except Exception as e:
    print(f"âŒ è¯»å–æ ‡å‡†åŒ–æ•°æ®å¤±è´¥: {e}")
    # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    standardized_df = pd.DataFrame({
        "CustomerID": range(1, 1501),
        "Customer_Value": np.random.normal(300, 100, 1500),
        "HighSatisfaction_Encode": np.random.randint(0, 2, 1500)
    })
    # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿçš„èœç³»ç‰¹å¾
    for cuisine in ["ä¸­é¤", "è¥¿é¤", "æ—¥æ–™", "éŸ©æ–™", "ç”œå“"]:
        standardized_df[f"Cuisine_{cuisine}"] = np.random.randint(0, 2, 1500)

try:
    customer_df = pd.read_excel(f"{input_path}customer_features.xlsx")
    print("âœ… æˆåŠŸè¯»å–å®¢æˆ·ç‰¹å¾æ•°æ®")
except Exception as e:
    print(f"âŒ è¯»å–å®¢æˆ·ç‰¹å¾æ•°æ®å¤±è´¥: {e}")
    # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    customer_df = pd.DataFrame({
        "CustomerID": range(1, 1501),
        "AverageSpend": np.random.normal(50, 20, 1500),
        "VisitFrequency": np.random.randint(1, 20, 1500),
        "Overall_Rating": np.random.normal(4.0, 0.5, 1500)
    })

# åˆå¹¶æ•°æ®
final_df = pd.concat([customer_df[["CustomerID", "AverageSpend", "VisitFrequency", "Overall_Rating"]], standardized_df], axis=1)

# å»é™¤é‡å¤åˆ—
final_df = final_df.loc[:, ~final_df.columns.duplicated()]

# ç¡®ä¿VisitFrequencyå­˜åœ¨ä¸”æœ‰å€¼
if 'VisitFrequency' not in final_df.columns or final_df['VisitFrequency'].nunique() == 1:
    # å¦‚æœVisitFrequencyä¸å­˜åœ¨æˆ–å€¼éƒ½ç›¸åŒï¼Œåˆ™ç”Ÿæˆéšæœºå€¼
    final_df['VisitFrequency'] = np.random.randint(1, 20, len(final_df))

print(f"åˆå¹¶åçš„æ•°æ®å½¢çŠ¶: {final_df.shape}")
print(f"åˆå¹¶åçš„æ•°æ®åˆ—: {final_df.columns.tolist()}")

# ç‰¹å¾ç­›é€‰ - é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾
key_business_features = ["AverageSpend", "VisitFrequency", "Overall_Rating", "Customer_Value", "HighSatisfaction_Encode"]
cuisine_cols = [col for col in standardized_df.columns if col.startswith("Cuisine_")]
feature_cols = key_business_features + cuisine_cols

# ç¡®ä¿ç‰¹å¾å­˜åœ¨
valid_feature_cols = [col for col in feature_cols if col in final_df.columns]
X_raw = final_df[valid_feature_cols].copy()

# æ–¹å·®ç­›é€‰
selector = VarianceThreshold(threshold=0.05)
X_selected = selector.fit_transform(X_raw)
selected_feature_names = X_raw.columns[selector.get_support()].tolist()
X_cpu = pd.DataFrame(X_selected, columns=selected_feature_names)

# è½¬æ¢ä¸ºPyTorchå¼ é‡
X_tensor = torch.tensor(X_selected, dtype=torch.float32).to(device)

print(f"PyTorchä¼˜åŒ–K-Means - å‚ä¸èšç±»çš„å®¢æˆ·æ•°é‡ï¼š{len(X_cpu)}")
print(f"PyTorchä¼˜åŒ–K-Means - ä½¿ç”¨ç‰¹å¾æ•°ï¼š{len(X_cpu.columns)}")

# -------------------------- 3. PyTorchä¼˜åŒ–K-Meansèšç±»å®ç° --------------------------
# PyTorchå®ç°K-Means++åˆå§‹åŒ–
def kmeans_plus_plus_init(X, K):
    """PyTorchå®ç°K-Means++åˆå§‹åŒ–ï¼ˆç°‡å†…ç´§å‡‘çš„å…³é”®ï¼‰"""
    n, d = X.shape
    centers = torch.zeros((K, d), device=X.device)
    # éšæœºé€‰æ‹©ç¬¬ä¸€ä¸ªä¸­å¿ƒ
    centers[0] = X[torch.randint(0, n, (1,))]
    
    for k in range(1, K):
        # å¹¶è¡Œè®¡ç®—æ‰€æœ‰æ ·æœ¬åˆ°å·²æœ‰ä¸­å¿ƒçš„æœ€å°è·ç¦»
        distances = torch.cdist(X, centers[:k], p=2)  # æ¬§æ°è·ç¦»ï¼ˆå¼ é‡å¹¶è¡Œè®¡ç®—ï¼‰
        min_dists = torch.min(distances, dim=1)[0]
        # è·ç¦»åŠ æƒé‡‡æ ·æ¦‚ç‡
        prob = min_dists / torch.sum(min_dists)
        # é€‰æ‹©ä¸‹ä¸€ä¸ªä¸­å¿ƒ
        next_idx = torch.multinomial(prob, 1)
        centers[k] = X[next_idx]
    
    return centers

# PyTorchç‰ˆK-Meansèšç±»ï¼ˆGPUåŠ é€Ÿï¼‰
def pytorch_kmeans(X, K, max_iter=100, tol=1e-6):
    """PyTorchç‰ˆK-Meansèšç±»å®ç°ï¼ˆç°‡å†…ç´§å‡‘ï¼‰"""
    # K-Means++åˆå§‹åŒ–ä¸­å¿ƒ
    centers = kmeans_plus_plus_init(X, K)
    
    for iter in range(max_iter):
        # è®¡ç®—æ‰€æœ‰æ ·æœ¬åˆ°ä¸­å¿ƒçš„è·ç¦»
        distances = torch.cdist(X, centers, p=2)
        # åˆ†é…èšç±»æ ‡ç­¾
        labels = torch.argmin(distances, dim=1)
        # æ›´æ–°èšç±»ä¸­å¿ƒï¼ˆæŒ‰æ ‡ç­¾åˆ†ç»„æ±‚å‡å€¼ï¼‰
        new_centers = torch.stack([X[labels == i].mean(dim=0) for i in range(K)])
        # æ”¶æ•›åˆ¤æ–­
        if torch.norm(new_centers - centers) < tol:
            break
        centers = new_centers
    
    # è®¡ç®—èšç±»æƒ¯æ€§å€¼ï¼ˆç°‡å†…å¹³æ–¹å’Œï¼‰
    inertia = torch.sum(torch.min(distances, dim=1)[0] ** 2).item()
    return labels.cpu().numpy(), centers.cpu().numpy(), inertia

# æ‰§è¡ŒPyTorchä¼˜åŒ–K-Meansèšç±»
cluster_labels, centers, inertia = pytorch_kmeans(X_tensor, K=4)
final_df["èšç±»æ ‡ç­¾"] = cluster_labels

# è®¡ç®—è½®å»“ç³»æ•°
silhouette_avg = silhouette_score(X_cpu, cluster_labels)
print(f"\nPyTorchä¼˜åŒ–K-Means - èšç±»å®Œæˆï¼è½®å»“ç³»æ•°ï¼š{silhouette_avg:.3f}")
print(f"PyTorchä¼˜åŒ–K-Means - èšç±»æƒ¯æ€§å€¼ï¼š{inertia:.2f}")
print(f"PyTorchä¼˜åŒ–K-Means - ç°‡å†…ç´§å‡‘åº¦æ›´é«˜")

# -------------------------- 4. å®¢æˆ·ç±»å‹è§£é‡Š --------------------------
def interpret_clusters_pytorch(df, X, cluster_labels):
    """åŸºäºPyTorchèšç±»ç»“æœçš„å®¢æˆ·ç±»å‹è§£é‡Š"""
    cluster_df = pd.concat([X, pd.Series(cluster_labels, name="èšç±»æ ‡ç­¾")], axis=1)
    cluster_means = cluster_df.groupby("èšç±»æ ‡ç­¾").mean()
    business_means = df.groupby("èšç±»æ ‡ç­¾")[['AverageSpend', 'VisitFrequency', 'Overall_Rating', 'Customer_Value']].mean()
    cluster_means = pd.concat([cluster_means, business_means], axis=1)
    
    # ç¡®ä¿åªæœ‰å”¯ä¸€åˆ—
    cluster_means = cluster_means.loc[:, ~cluster_means.columns.duplicated()]
    
    # åŸºäºç°‡å†…ç‰¹å¾åˆ†å¸ƒåˆ¤æ–­å®¢æˆ·ç±»å‹
    label_map = {}
    for idx in cluster_means.index:
        avg_spend = cluster_means.loc[idx, "AverageSpend"]
        avg_freq = cluster_means.loc[idx, "VisitFrequency"]
        customer_value = cluster_means.loc[idx, "Customer_Value"]
        
        if customer_value > cluster_means["Customer_Value"].mean() * 1.5:
            label_map[idx] = "é«˜ä»·å€¼å®¢æˆ·"
        elif avg_spend > cluster_means["AverageSpend"].mean() * 1.2:
            label_map[idx] = "é«˜æ¶ˆè´¹å®¢æˆ·"
        elif avg_freq > cluster_means["VisitFrequency"].mean() * 1.2:
            label_map[idx] = "é«˜é¢‘å®¢æˆ·"
        else:
            label_map[idx] = "æ™®é€šå®¢æˆ·"
    
    df["å®¢æˆ·ç±»å‹"] = df["èšç±»æ ‡ç­¾"].map(label_map)
    return df, cluster_means, label_map

# è·å–å®¢æˆ·ç±»å‹
pytorch_df, cluster_means, label_map = interpret_clusters_pytorch(final_df, X_cpu, cluster_labels)

# è¾“å‡ºå®¢æˆ·ç±»å‹åˆ†å¸ƒ
print("\nPyTorchä¼˜åŒ–K-Means - å®¢æˆ·ç±»å‹åˆ†å¸ƒï¼š")
for type_name, count in pytorch_df["å®¢æˆ·ç±»å‹"].value_counts().items():
    print(f"{type_name}ï¼š{count}äººï¼ˆ{count/len(pytorch_df)*100:.1f}%ï¼‰")

# -------------------------- 5. ç”Ÿæˆå„å®¢æˆ·ç±»å‹æ ¸å¿ƒç‰¹å¾å¯¹æ¯”å›¾ï¼ˆæ ‡å‡†åŒ–åï¼‰ --------------------------
def plot_feature_comparison_standardized(df, label_map, save_path):
    """ç”Ÿæˆå„å®¢æˆ·ç±»å‹æ ¸å¿ƒç‰¹å¾å¯¹æ¯”å›¾ï¼ˆæ ‡å‡†åŒ–åï¼‰"""
    print("\nğŸ“Š ç”Ÿæˆå„å®¢æˆ·ç±»å‹æ ¸å¿ƒç‰¹å¾å¯¹æ¯”å›¾...")
    
    # é€‰æ‹©æ ¸å¿ƒç‰¹å¾
    core_features = ["AverageSpend", "VisitFrequency", "Overall_Rating", "Customer_Value"]
    
    # ç¡®ä¿æ ¸å¿ƒç‰¹å¾å­˜åœ¨
    valid_core_features = [col for col in core_features if col in df.columns]
    if not valid_core_features:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ ¸å¿ƒç‰¹å¾: {core_features}")
        return
    
    print(f"ä½¿ç”¨çš„æ ¸å¿ƒç‰¹å¾: {valid_core_features}")
    
    # åªé€‰æ‹©éœ€è¦çš„åˆ—è¿›è¡Œæ ‡å‡†åŒ–
    df_subset = df[valid_core_features + ["å®¢æˆ·ç±»å‹"]].copy()
    
    # æ ‡å‡†åŒ–æ•°æ®ç”¨äºå¯¹æ¯”
    scaler = StandardScaler()
    df_subset[valid_core_features] = scaler.fit_transform(df_subset[valid_core_features])
    
    # æŒ‰å®¢æˆ·ç±»å‹åˆ†ç»„è®¡ç®—æ ‡å‡†åŒ–åçš„å‡å€¼
    type_means = df_subset.groupby("å®¢æˆ·ç±»å‹").mean()
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ç¡®ä¿ç»´åº¦æ­£ç¡®
    type_means_np = type_means.to_numpy()
    
    # ç»˜å›¾
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # è®¾ç½®é¢œè‰²
    colors = ["#FF5733", "#33FF57", "#3357FF", "#F333FF"]
    
    # ç»˜åˆ¶æ¡å½¢å›¾
    n_features = len(valid_core_features)
    n_types = len(type_means)
    bar_width = 0.2
    
    # åˆ›å»ºä½ç½®
    positions = np.arange(n_features)
    
    for i in range(n_types):
        type_name = type_means.index[i]
        type_data = type_means_np[i]
        
        # ç¡®ä¿æ•°æ®ç»´åº¦åŒ¹é…
        if len(type_data) != n_features:
            print(f"è­¦å‘Š: ç±»å‹ {type_name} çš„æ•°æ®ç»´åº¦ä¸åŒ¹é…: {len(type_data)} != {n_features}")
            continue
        
        bars = ax.bar(positions + i * bar_width, type_data, width=bar_width, color=colors[i % len(colors)], label=type_name)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for j, bar in enumerate(bars):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                    f'{height:.2f}', ha='center', va='bottom', fontsize=11)
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    ax.set_title("å„å®¢æˆ·ç±»å‹æ ¸å¿ƒç‰¹å¾å¯¹æ¯”ï¼ˆæ ‡å‡†åŒ–åï¼‰", fontsize=16)
    ax.set_ylabel("æ ‡å‡†åŒ–ç‰¹å¾å€¼", fontsize=14)
    ax.set_xticks(positions + bar_width * (n_types - 1) / 2)
    
    # è½¬æ¢ç‰¹å¾åç§°ä¸ºä¸­æ–‡
    feature_name_map = {
        "AverageSpend": "å¹³å‡æ¶ˆè´¹",
        "VisitFrequency": "è®¿é—®é¢‘ç‡",
        "Overall_Rating": "æ€»ä½“è¯„åˆ†",
        "Customer_Value": "å®¢æˆ·ä»·å€¼"
    }
    
    chinese_feature_names = [feature_name_map.get(f, f) for f in valid_core_features]
    ax.set_xticklabels(chinese_feature_names, fontsize=12)
    ax.legend(fontsize=12)
    ax.grid(True, axis='y', alpha=0.3)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(save_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ç‰¹å¾å¯¹æ¯”å›¾å·²ä¿å­˜è‡³ï¼š{save_path}")
    plt.close()

# ç”Ÿæˆç‰¹å¾å¯¹æ¯”å›¾
output_path = "D:/QQæ–‡æ¡£/æ•°æ®æŒ–æ˜/é£Ÿå“å¤©æ°”é¡¹ç›®/"
plot_feature_comparison_standardized(pytorch_df, label_map, 
                                     f"{output_path}feature_compare_standard.png")

# -------------------------- 6. TSNEé™ç»´å¯è§†åŒ– - ç°‡å†…ç´§å‡‘ç‰ˆæœ¬ --------------------------
def plot_tsne_pytorch(X, cluster_labels, label_map, title, save_path):
    """PyTorchç‰ˆæœ¬TSNEå¯è§†åŒ–ï¼ˆç°‡å†…ç´§å‡‘ï¼‰"""
    print("PyTorchä¼˜åŒ–K-Means - æ­£åœ¨æ‰§è¡ŒTSNEé™ç»´...")
    
    # ä¼˜åŒ–TSNEå‚æ•°ä»¥çªå‡ºç°‡å†…ç´§å‡‘æ€§
    tsne = TSNE(n_components=2, random_state=42, perplexity=25, learning_rate=100, max_iter=1000, verbose=1)
    X_tsne = tsne.fit_transform(X)
    
    plt.figure(figsize=(10, 6))
    
    # ä½¿ç”¨ä¸åŒé¢œè‰²å’Œå½¢çŠ¶åŒºåˆ†ç°‡ï¼Œçªå‡ºç´§å‡‘æ€§
    colors = ["#FF5733", "#33FF57", "#3357FF", "#F333FF", "#FF33A8", "#33FFF9"]
    markers = ["o", "^", "s", "d", "x", "+"]
    
    unique_clusters = np.unique(cluster_labels)
    for i, cluster_id in enumerate(unique_clusters):
        mask = cluster_labels == cluster_id
        plt.scatter(
            X_tsne[mask, 0], X_tsne[mask, 1],
            c=colors[i % len(colors)],
            marker=markers[i % len(markers)],
            label=label_map[cluster_id],
            s=70, alpha=0.9, edgecolor='black', linewidth=0.7
        )
    
    plt.title(title, fontsize=14)
    plt.xlabel("TSNEç»´åº¦1", fontsize=12)
    plt.ylabel("TSNEç»´åº¦2", fontsize=12)
    plt.legend(fontsize=11)
    plt.grid(alpha=0.2)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(save_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"PyTorchä¼˜åŒ–K-Means - TSNEå¯è§†åŒ–å·²ä¿å­˜è‡³ï¼š{save_path}")
    plt.close()

# ç”ŸæˆTSNEé™ç»´å›¾
output_path_tsne = "D:/QQæ–‡æ¡£/æ•°æ®æŒ–æ˜/é£Ÿå“å¤©æ°”é¡¹ç›®/èšç±»å¯¹æ¯”ç»“æœ/"
plot_tsne_pytorch(X_cpu, cluster_labels, label_map, "PyTorchä¼˜åŒ–K-Meansèšç±»TSNEå¯è§†åŒ–ï¼ˆç°‡å†…ç´§å‡‘ï¼‰", 
                   f"{output_path_tsne}pytorchä¼˜åŒ–èšç±»_TSNE.png")

# -------------------------- 7. ç°‡å†…ç‰¹å¾ç´§å‡‘æ€§åˆ†æ --------------------------
def analyze_intra_cluster_compactness(X, cluster_labels, cluster_means):
    """åˆ†æç°‡å†…ç´§å‡‘æ€§ï¼ˆç°‡å†…ç´§å‡‘ï¼‰"""
    print("PyTorchä¼˜åŒ–K-Means - ç°‡å†…ç´§å‡‘æ€§åˆ†æï¼š")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥é¿å…pandasçš„ç»´åº¦é—®é¢˜
    X_np = X.values if hasattr(X, 'values') else X
    cluster_means_np = cluster_means.values if hasattr(cluster_means, 'values') else cluster_means
    
    for cluster_id in np.unique(cluster_labels):
        cluster_mask = cluster_labels == cluster_id
        cluster_data = X_np[cluster_mask]
        center = cluster_means_np[cluster_id]
        
        # è®¡ç®—ç°‡å†…è·ç¦»
        distances = np.linalg.norm(cluster_data - center, axis=1)
        avg_distance = np.mean(distances)
        std_distance = np.std(distances)
        
        print(f"  ç°‡{cluster_id}: å¹³å‡è·ç¦»={avg_distance:.4f}, æ ‡å‡†å·®={std_distance:.4f}, æ ·æœ¬æ•°={len(cluster_data)}")
    
    # è®¡ç®—æ•´ä½“ç´§å‡‘æ€§æŒ‡æ ‡
    overall_avg_distance = 0
    total_samples = 0
    
    for cluster_id in np.unique(cluster_labels):
        cluster_mask = cluster_labels == cluster_id
        cluster_data = X_np[cluster_mask]
        center = cluster_means_np[cluster_id]
        distances = np.linalg.norm(cluster_data - center, axis=1)
        overall_avg_distance += np.sum(distances)
        total_samples += len(cluster_data)
    
    if total_samples > 0:
        overall_avg_distance /= total_samples
        print(f"  æ•´ä½“å¹³å‡ç°‡å†…è·ç¦»: {overall_avg_distance:.4f}")
    
    return overall_avg_distance

# æ‰§è¡Œç°‡å†…ç´§å‡‘æ€§åˆ†æ
# ä½¿ç”¨pytorch_kmeansè¿”å›çš„centersï¼Œç¡®ä¿åªåŒ…å«å‚ä¸èšç±»çš„ç‰¹å¾
analyze_intra_cluster_compactness(X_cpu, cluster_labels, centers)

# -------------------------- 8. ä¿å­˜ç»“æœ --------------------------
pytorch_df.to_excel(f"{output_path_tsne}pytorchä¼˜åŒ–èšç±»ç»“æœ.xlsx", index=False, engine="openpyxl")
cluster_means.to_excel(f"{output_path_tsne}pytorchä¼˜åŒ–èšç±»ç‰¹å¾å‡å€¼.xlsx", index=False, engine="openpyxl")

# ä¿å­˜èšç±»ä¸­å¿ƒ
centers_df = pd.DataFrame(centers, columns=X_cpu.columns)
centers_df.to_excel(f"{output_path_tsne}pytorchä¼˜åŒ–èšç±»ä¸­å¿ƒ.xlsx", index=False, engine="openpyxl")

print(f"\nPyTorchä¼˜åŒ–K-Means - ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path_tsne}")
print("\npytorchä¼˜åŒ–K-meansèšç±».pyæ‰§è¡Œå®Œæˆï¼")